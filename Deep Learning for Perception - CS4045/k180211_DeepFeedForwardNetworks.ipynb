{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYUH0qf8Dqkm"
   },
   "source": [
    "Deep Feed-forward Networks\n",
    "\n",
    "Â© Deep Learning Indaba. Apache License 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hH4wd81A_P8"
   },
   "source": [
    "## Introduction\n",
    "In this practical, we implement and train a feed-forward neural network (also known as an \"MLP\" for \"multi-layer perceptron\") on a dataset called \"Fashion MNIST\", consisting of small greyscale images of items of fashion. We consider the practical issues around generalisation to out-of-sample data and introduce some important techniques for addressing this. \n",
    "\n",
    "## Learning Objectives \n",
    "* Understand how a feed forward neural network model is trained and evaluated\n",
    "* Be able to explain why there is a difference between **in-sample (data points used for training)** and **out-of-sample (data points used for testing)** model performance\n",
    "* Understand the concept of **train/validation/test split** and why it's useful\n",
    "* Analyze the effect of different optimzers on learning curve and performance of the model.\n",
    "* Research at least 1 technique that can be used to improve model **generalisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxSD3yPDfEIj"
   },
   "outputs": [],
   "source": [
    "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
    "\n",
    "!pip install tensorflow-gpu==2.0.0-beta0 > /dev/null 2>&1\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG4HfHz5tMce"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons\n",
    "import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU33KeDUDomX"
   },
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL0dFs-DD6sM"
   },
   "source": [
    "In this practical, we use the Fashion MNIST dataset consisting of 70,000 greyscale images and their labels. The dataset is divided\n",
    " into 60,000 training images and 10,000 test images. The idea is to train a **classifier** to identify the class value (what type of fashion item it is) given the image. We train and *tune* a model on the 60,000 training images and then evaluate how well it classifies the 10,000 test images that the model did not see during training. This task is an example of a **supervised learning** problem, where we are given both input and labels (targets) to learn from. This is in contrast to **unsupervised learning** where we only have inputs from which to learn patterns or **reinforcement learning** where an agent learns how to maximise a reward signal through interaction with its environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VRxcYWHfZJ5"
   },
   "outputs": [],
   "source": [
    "# Tensorflow has convenient modules for loading a number of standard datasets\n",
    "import tensorflow as tf\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_and_validation_images, train_and_validation_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "text_labels = {0:'T-shirt/top', 1:'Trouser', 2:'Pullover', 3:'Dress', 4:'Coat', 5:'Sandal', 6:'Shirt', 7:'Sneaker', 8:'Bag', 9:'Ankle boot'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lEM3aBa2N63"
   },
   "source": [
    "###Optional extra reading: Train/Validation/Test Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6lib7RyVqYV"
   },
   "source": [
    "When we build machine learning models, the goal is to build a model that will perform well on *future* data that we have not seen yet. We say that we want our models to be able to **generalise** well from whatever training data we can collect and do have available, to whatever data we will be applying them to in future. To do this, we split whatever data we have available into a **training set, a validation set and a test set**. The idea is that we train our model and use the performance on the validation set to make any adjustments to the model and its hyperparameters, but then we report the final accuracy on the test set. The test set (which we never train on), therefore acts as a proxy for our future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z_Ks31qEfUM"
   },
   "source": [
    "### Construct a validation set\n",
    "Next, we remove 10,000 images and labels from the training set to use as a validation set. We will come back to the validation set later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZKEjjGM8I4t"
   },
   "outputs": [],
   "source": [
    "# Construct a validation set from the last 10000 images and labels from\n",
    "# train_and_validation_images and train_and_validation_labels. Also preprocess the data\n",
    "validation_images = train_and_validation_images[-10000:, :, :]/255 #normalizing the input data\n",
    "validation_labels = train_and_validation_labels[-10000:]\n",
    "\n",
    "\n",
    "# Construct a training set from the first 50000 images and labels. Also, preprocess the data\n",
    "train_images = train_and_validation_images[:50000, :, :]/255\n",
    "train_labels = train_and_validation_labels[:50000]\n",
    "\n",
    "\n",
    "#preprocess test dataset\n",
    "test_images=test_images/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbWk7qJOEkl6"
   },
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 28 x 28 matrix of greyscale pixels. The values are between 0 and 255 where 0 represents black, 255 represents white and there are many shades of grey in-between. Each image is assigned a corresponding numerical label, so the image in ```train_images[i]``` has its corresponding label stored in ```train_labels[i]```. We also have a lookup dictionary called ```text_labels``` to associate a text description with each of the numerical labels. For example, the label 1 is associated with the text description \"Trouser\".\n",
    "\n",
    "The 2 cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgfvNeX2gcAL"
   },
   "outputs": [],
   "source": [
    "# We use the Matplotlib plotting library to visualise an image selected at random from the training set\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "random_index = np.random.randint(0, train_images.shape[0])\n",
    "plt.imshow(train_images[random_index], cmap='gray_r')\n",
    "plt.colorbar()\n",
    "numerical_label = train_labels[random_index]\n",
    "text_description = text_labels[numerical_label]\n",
    "plt.title('True Class: {} (\"{}\")'.format(numerical_label, text_description))\n",
    "\n",
    "plt.gca().grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-utfMlxhhL3b"
   },
   "outputs": [],
   "source": [
    "# Another view, showing 25 randomly selected images at a time\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = np.random.randint(0,train_images.shape[0])\n",
    "    plt.imshow(train_images[img_index], cmap=\"gray_r\")\n",
    "    plt.xlabel(text_labels[train_labels[img_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMwKN4DHcqXV"
   },
   "source": [
    "### Exploratory Task\n",
    "Run the above cells multiple times to get a good sense of the dataset. Look at the general structure of the images and their corresponding labels. Can you spot some classes that might be difficult for a classifier to distinguish? If so, why do you think they would be difficult to distinguish? Chat to your neighbour about this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6U63sTpGYvg"
   },
   "source": [
    "### Preparing the data with TensorFlow\n",
    "At the moment, our training data consists of two large tensors. The images are stored in a tensor of shape $[50000, 28, 28]$, consisting of all the $28 \\times 28$ images matrices stacked together. The labels are stored in a 1D vector of shape $[50000]$. We wish to train a model using **mini-batch stochastic gradient descent**. In order to do so, we need to shuffle the data and split it into smaller (mini-)batches. We also convert the data from numpy arrays to TensorFlow Tensors.\n",
    "\n",
    "#### **Questions 1**\n",
    "* a. Can you explain what the difference is between normal stochastic gradient descent and \"batched\" stochastic gradient descent? What is the purpose of chunking data into batches during training? Are there any trade-offs? What are they?\n",
    "* b. Why is it important to randomise the data before chunking it into batches? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bukK3DDN1Lh8"
   },
   "source": [
    "a). The batched stochastic gradient descent uses batches of data during training instead of taking whole sample as oppose to normal stochastic gradient descent. The batch stochastic gradient descent is light on memory, therefore it is suggested\n",
    "\n",
    "b). Inorder to avoid biasness we need to randomise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v04ibRFSALQu"
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KItI--nMHFRQ"
   },
   "source": [
    "In this section we'll build a classifier. A **classifier** is a function that takes an object's characteristics (or \"features\") as inputs and outputs a prediction of the class (or group) that the object belongs to. It may make a single prediction for each input or it may output some score (for example a probability) for each of the possible classes. Specifically, we will build a classifier that takes in (a batch of) 28 x 28 Fashion MNIST images as we've seen above, and outputs predictions about which class the image belongs to. \n",
    "\n",
    "For each (batch of) input images, we will use a **feed-forward neural network** to compute un-normalised scores (also known as **logits**) for each of the 10 possible classes that the image could belong to. We can then **classify** the image as belonging to the class which receives the highest score, or we can quantify the model's \"confidence\" about the classifications by converting the scores into a probability distribution. \n",
    "\n",
    "A feed-forward neural network consisting of $N$ layers, applied to an input vector $\\mathbf{x}$ can be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{f_0} = \\mathbf{x} \\\\\n",
    "\\mathbf{f_i} = \\sigma_i(\\mathbf{W_if_{i-1}} + \\mathbf{b_i}) \\ \\ \\ i \\in [1, N]\n",
    "\\end{equation}\n",
    "\n",
    "Each layer has a particular number, $m_i$, of neurons. The parameters of a layer consist of a matrix $\\mathbf{W_i} \\in \\mathbb{R}^{m_i \\times m_{i-1}}$ and bias vector $\\mathbf{b_i} \\in \\mathbb{R}^{m_i}$. Each layer also has a non-linear activation function $\\sigma_i$. \n",
    "\n",
    "**QUESTION 2**: Why do you think the activation functions need to be *non-linear*? What would happen if they were *linear*? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND6_jbs42AbB"
   },
   "source": [
    "The activation functions need to be non-linear because there is non-linear relationship, inorder to classify the data points the non linear curve would be needed, The linear function would not be able to make a linearly seperable line  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHy4bjbTVCzV"
   },
   "source": [
    "### Optional extra reading: Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox7VKs3BVCFV"
   },
   "source": [
    "\n",
    "\n",
    "Activation functions are a core ingredient in deep neural networks. In fact they are what allows us to make use of multiple layers in a neural network. There are a number of different activation functions, each of which are more or less useful under different circumstances. The four activation functions that you are most likely to encounter are, arguably, [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU), [Tanh](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh), [Sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid), and [Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax). \n",
    "\n",
    "ReLU, has in recent years, become the default choice for use in MLPs and Convolutional Neural Networks (CNNs). ReLU has two advantages over Tanh and Sigmoid: it is computationally much more efficient, and, it allows us to use deeper networks because it does not suffer from [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). As a result of their success, a number of ReLU variants, such as [LeakyRelu](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU) and [PReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU), have been developed.\n",
    "\n",
    "Sigmoid and Softmax activations are often found after the last layer in binary and multi-class classification networks, respectively, as they transform the outputs of the network in such a way that we can interpret them as class probabilities.\n",
    "\n",
    "Both Tanh and Sigmoid are found in LSTM and GRU recurrent neural networks, which we will find out more about in the coming days. They are also useful in MLPs and CNNs where we want the output to be bounded between -1 and 1 (Tanh) or 0 and 1 (Sigmoid).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDRoliDeAShd"
   },
   "source": [
    "### Configure the feed-forward neural network\n",
    "We configure the feed-forward neural network part of our classifier using the [Keras Layers API](https://www.tensorflow.org/api_docs/python/tf/keras/layers). This API consists of various reusable building-blocks that allow us to define many different neural network architectures (similar to how we defined a data pipeline earlier!). \n",
    "\n",
    "Here we use the [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) component which allows us to wrap together a sequence of layers. An important point to note here is that we are **configuring** our neural network architecture as a pipeline. We can think of the resulting ```model`` variable as a *function* that takes a batch of images as inputs and outputs a batch of logits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3AU_94YP1st"
   },
   "source": [
    "The following summary shows how many parameters each layer is made up of (the number of entries in the weight matrics and bias vectors). Note that a value of ```None``` in a particular dimension of a shape means that the shape will dynamically adapt based on the shape of the inputs. In particular, the output shape of the ```flatten_input``` layer will be $[N, 784]$ when the batch of inputs passed to the model has shape $[N, 28, 28]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwWoSlpDMzNZ"
   },
   "source": [
    "### Define the loss\n",
    "As we did in the previous practical, we need to specify a loss function for our classifier. This tells us how good our model's predictions are compared to the actual labels (the targets), with a lower loss meaning better predictions. The standard loss function to use with a **multi-class classifier** is the **cross-entropy loss** also known as the \"negative log likelihood\" for a classifier. Suppose we have a classification problem with $C$ classes. A classifier is trained to predict a probability distribution $p(y | X_i)$ for each input $X_i$ from a batch of $N$ examples. The vector $p(y|X_i)$ is $C$ dimensional, sums to one, and we use $p(y|X_i)_c$ to denote the $c$th component of  $p(y|X_i)$. The true class for example $i$ in the batch is $y_i$ and we define the indicator function $\\mathbb{1}[y_i=c]$ to be 1 whenever $y_i = c$ and $0$ otherwise. This classifier has a cross-entropy loss of\n",
    "\n",
    "$- \\frac{1}{N}\\sum_{i=1}^N \\sum_{c=1}^C log( p(y|X_i)_c) \\mathbb{1}[y_i=c]$\n",
    "\n",
    "**NOTE**: The indicator is one for the true class label, and zero everywhere else. So in that sum, the indicator just \"lifts out\" the $log(p(y|X_i))$ values for all true classes. So the above expression is minimised (note the negative at the front) when the model places all its probability mass on the true labels for all the examples. Remember that  log(1)=0 , thus the closer all probabilities of $y_i = c$ are to one, the lower the loss will be and the better the model will be performing.\n",
    "\n",
    "```tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)```. \n",
    "\n",
    "**NOTE**: This function actually computes the cross entropy loss directly from the un-normalised logits, rather than from the probability distribution for numerical stability.\n",
    "\n",
    "By the way, for training data in which the labels are themselves distributions rather than exact values, this definition of cross-entropy still works, where the indicator function is replaced with the corresponding probability of each class for that example. This might be important when we are not sure whether the training data has been labelled correctly, or when the data was labelled by a human who gave their answer along with a degree of confidence that the answer was correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq74lysqP7AP"
   },
   "source": [
    "**QUESTION 03**: \n",
    "* a. Why do you think this is a *good* loss function?\n",
    "* b. Can you think of any potential issues with this loss function?\n",
    "\n",
    "Fortunately we don't need to write this function ourselves as Tensorflow provides a version called "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHfd_ibT6zIO"
   },
   "source": [
    "a). The sparsecategoricalcrossentropy is good in this case because it directly computes loss from unnormalised logits so less computation is required\n",
    "\n",
    "b). There is a need to do one hot encoding in categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDPrTMSahRBu"
   },
   "outputs": [],
   "source": [
    "# RE-RUN THIS CELL if you want to restart training!\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Convert the 28x28 image into a flat vector of 28x28 = 784 values\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28), name='flatten_input')),\n",
    "    # Create a \"hidden\" layer with 256 neurons and apply the ReLU non-linearity\n",
    "    model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu, name='input_to_hidden1')),\n",
    "    # Create another hidden layer with 128 neurons\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu, name='hidden1_to_hidden2')),\n",
    "    # Create an \"output layer\" with 10 neurons\n",
    "    model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax, name='hidden_to_logits')),\n",
    "    model.compile(optimizer=tf.optimizers.SGD(),  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['SparseCategoricalCrossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OHi-HftJy9_"
   },
   "source": [
    "## Train the model\n",
    "Now that we have our data, data processing pipeline, our neural network architecture and the corresponding loss that we want to minimise, we need to **train** the model using batched stochastic gradient descent. We do this in multiple **epochs**, which is a single iteration through the entire training dataset. Briefly, in each epoch we loop over all the batches of images and labels, and for each batch we perform the following steps:\n",
    "* Get the **predictions** of the model on the current batch of images\n",
    "* Compute the **average loss** values across the batch, telling us how good these predictions are / how close they are to the true targets.\n",
    "* Compute the **gradient of the average loss** (or the average gradient of the losses in the batch) with respect to each of the model's parameters: This tells us the direction to move in \"parameter space\" to decrease the loss value\n",
    "* **Adjust the parameters** by taking a small step in the direction of each component of the gradient (where the learning rate controls the size of the step)\n",
    "\n",
    "During training we also track some metrics, such as the loss and accuracy to see how well the classifier is doing. Note that the cell below may take a few minutes to run! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJdxpdGdoXmn"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwI8FcreWsC_"
   },
   "source": [
    "**QUESTION 4**: Note that the flattening operation has no parameters. However, the Dense (fully-connected) layer mapping from the 784-D output of the flatten operation to the 256-D hidden layer has 200,960 parameters! Can you explain why the dense layer has 200,960 parameters? **HINT**: Refer back to the equations of a feed-forward network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0pG5Q6qBuXX"
   },
   "source": [
    "784*256 + 256(bias) would give 200960 parameters, as the from flatten layer would go to every input_to_hidden layer neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSJnC50RykAc"
   },
   "outputs": [],
   "source": [
    "# initialize tqdm callback with default parameters\n",
    "tqdm_callback = tfa.callbacks.TQDMProgressBar(show_epoch_progress = False,)\n",
    "history = model.fit(train_images, train_labels, epochs=50, batch_size=32, verbose=0,  validation_split = 0.0, callbacks=[tqdm_callback ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd_pd3Jiul9E"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0SAv2g0fbiF"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot the loss for all epochs using Matplotlib\n",
    "plt.figure()\n",
    "plt.plot(hist.epoch+1, hist.loss, '-o')\n",
    "plt.title('Loss vs epochs')\n",
    "\n",
    "# Plot the accuracy for all epochs using Matplotlib\n",
    "plt.figure()\n",
    "plt.plot(hist.epoch+1, hist.accuracy, '-o')\n",
    "plt.title('Accuracy vs epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-oib_tntPIY"
   },
   "source": [
    "The above code block shows a typical training loop. There's actually an easier way to do this using Tenosrflow and Keras, which we'll see in the next prac, but for now we expand it so you can see what's going on.\n",
    "\n",
    "Lets visualise some of the model's prediction on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rotl5kAX7SrW"
   },
   "outputs": [],
   "source": [
    "rnd=np.random.randint(0, train_images.shape[0],25)\n",
    "images, labels = train_images[rnd], train_labels[rnd]\n",
    "\n",
    "_logits = model(images, training=False)  # Pass the images to the model function and get its output logits\n",
    "predicted_labels = tf.argmax(_logits, axis=1, output_type=tf.int32)\n",
    "\n",
    "img_indexs = np.arange(images.shape[0])\n",
    "np.random.shuffle(img_indexs)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = img_indexs[i]\n",
    "    predicted_label = int(predicted_labels[img_index])\n",
    "\n",
    "    plt.imshow(images[img_index], cmap=\"gray_r\")\n",
    "\n",
    "    actual_label = int(labels[img_index])\n",
    "    plt.xlabel(\"Actual: {} ({})\\n Predicted: {} ({})\".format(actual_label, text_labels[actual_label], predicted_label, text_labels[predicted_label]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMwZVX1T3zWP"
   },
   "source": [
    "## Testing\n",
    "\n",
    "After 50 epochs of training on our training set, we obtain a loss of around 0.061 and accuracy of 97.7%. That's pretty good, right? \n",
    "\n",
    "It is important to distinguish between data that is **in-sample** and **out-of-sample**. Our training data is all in-sample and we would expect any resonably powerful model (like an MLP) to be very accurate and have a low loss on this (in fact, with a sufficiently large MLP and enough training epochs, we can get the loss arbitrarily close to zero!). This is not a good thing though, because pushing the loss close to zero may mean that the model has fit the **noise** in the training data rather than the true signal. If this is the case, it will not perform well on out-of-sample data that it has not seen before (which is what we really care about!). To assess this, we evaluate our trained model on the held-out test set (but we don't update the parameters of the model when testing it, that would be cheating!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFMiN9Bc1Jo9"
   },
   "outputs": [],
   "source": [
    "# model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0BaJidP46Bm"
   },
   "outputs": [],
   "source": [
    "print(test_images.shape)\n",
    "_,test_loss, test_accuracy= model.evaluate(test_images,test_labels)\n",
    "print('Completed testing on', test_images.shape[0], 'examples...')\n",
    "print('Loss: {:.3f}, Accuracy: {:.3%}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn4ptik28aHF"
   },
   "source": [
    "And again we visualise some of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHVSTjZU8ehm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_predictions = tf.argmax(model.predict(test_images), axis=1, output_type=tf.int32)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = np.random.randint(0, 10000)\n",
    "    plt.imshow(test_images[img_index], cmap=\"gray_r\")\n",
    "\n",
    "    actual_label = test_labels[img_index]\n",
    "    predicted_label = int(test_predictions[img_index])\n",
    "\n",
    "    plt.xlabel(\"Actual: {} ({})\\n Predicted: {} ({})\".format( actual_label, text_labels[actual_label], predicted_label, text_labels[predicted_label]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo6bEKs17a_d"
   },
   "source": [
    "What happened? You got accuracy on the training set much larger as compared to that on the test set> This is an example of how a model's in-sample performance differs from its out-of-sample performance. It may even be considered **overfitting** (where the model failing to generalise its in-sample performance to unseen out-of-sample data). There are a number of ways we can address this, but first, it's important to realise that we should try and keep the number of evaluations on the test set to a minimum. \n",
    "\n",
    "The first important method is called **validation**. Using validation, we reserve a portion of our training dataset and call it the validation set. After each epoch of training (or every K epochs where K can be a fraction), we evaluate our model on this validation set (without updating the model parameters). This gives us a good sense of how well we expect our model to do on out-of-sample data. In this way, the validation set acts as a surrogate for the test set which we can safely use during model development and training. We can also use this validation set to tune our model by selecting hyperparameters such as the batch size, sizes of layers or the network architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIUsSjNwVk_n"
   },
   "source": [
    " ### Optional extra reading: Optimisation schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExBhca58VpPH"
   },
   "source": [
    "There is a whole range of variations on stochastic gradient descent that are often used. Here is an illustration showing how a few of these methods perform on a toy problem: \n",
    "\n",
    "![optimization methods](http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot-FjimzV6op"
   },
   "source": [
    "### Optional extra reading: Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQMKAkkFV7bf"
   },
   "source": [
    "In addition to the problem of _overfitting_ (in which the model is too complex and as a result is able to memorize the training data rather than learning a general pattern) there is the problem of _underfitting_ which is the opposite. Underfitting occurs when the model does not have enough complexity (also known as capacity) to learn a general pattern. While overfitting is characterised by diverging test/validation and training scores, underfitting is characterised by test/validation and training scores that very slowly continue to improve. \n",
    "\n",
    "\n",
    "Unlike overfitting which has a number of solutions, such as collecting more data, data augmentation, dropout, and L1/L2 regularization, underfitting has one simple solution: just make your model incrementally more complex until it no longer underfits.\n",
    "\n",
    "![underandoverfitting](https://i.imgur.com/m2bSP1S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8KSIHzkWC1N"
   },
   "source": [
    "##TASKS:\n",
    "\n",
    "1.Implement **Validation** by adding some code to the loop in the \"Train the model\" section (**TIP**: The code for evaluating on the validation set will be very similar to evaluating on the test set!): \n",
    " * a. Evaluate your model on the validation set at the end of each training epoch and print the loss and accuracy on the validation set. We have already defined a validation set and done the necessary preprocessing for you. What do you notice?\n",
    " * b. Plot the validation loss and accuracy on the same axes as the training loss and accuracy, respectively.\n",
    "  * c. Evaluate how closely the loss and accuracy on the validation set matches the eventual performance on the test set. \n",
    "\n",
    "2. Remove the shuffling from the dataset pipeline and compare the generalisation performance of the model with and without shuffling. \n",
    "\n",
    "3. Try different optimizers and compare the results.\n",
    "\n",
    "3. Adjust the model architecture by adding layers and modifying other parameters to improve the performance on your **validation** set. Once you've settled on an architecture and hyperparameters, evaluate your model on the **test** set. See the Tensorflow documentation for examples of parameters you can change in the model.\n",
    "\n",
    "4. [**OPTIONAL**] Add DROPOUT (see [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) and [research paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) ) to improve the model's generalisation.\n",
    "\n",
    "5. [**OPTIONAL**] Add BATCH NORMALISATION ([Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) and [research paper](http://proceedings.mlr.press/v37/ioffe15.pdf)) to improve the model's generalisation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Important Tip\n",
    "If you want to re-train your model, reset and run all cells again. This is to ensure that the Tensorflow variables containing the model parameters get reset. If you don't do this, you're effectively running more epochs on an already-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRft7qCI5RuZ"
   },
   "source": [
    "**Validation with 30% data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGu4d7Fh43NT"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=50, batch_size=32, verbose=0,  validation_split = 0.3, callbacks=[tqdm_callback ])\n",
    "\n",
    "# hist = pd.DataFrame(history.history)\n",
    "# hist['epoch'] = history.epoch\n",
    "# hist.tail()\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# I notice that the model is overfitting after 15 epochs, as it is giving good accuracy on \n",
    "#training data while the accuracy for validation data is not increasing after 10 epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL8KQvZv8DFE"
   },
   "source": [
    "**removing shuffling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uYvQYuXGa5j"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=50, batch_size=32, verbose=0, shuffle=False,  validation_split = 0.3, callbacks=[tqdm_callback ])\n",
    "\n",
    "# hist = pd.DataFrame(history.history)\n",
    "# hist['epoch'] = history.epoch\n",
    "# hist.tail()\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#I notice that without shuffling the training got worst "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJQ8esWE8onW"
   },
   "source": [
    "**changing optimizer to adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qghxTEaL8n5p"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Convert the 28x28 image into a flat vector of 28x28 = 784 values\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28), name='flatten_input')),\n",
    "    # Create a \"hidden\" layer with 256 neurons and apply the ReLU non-linearity\n",
    "    model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu, name='input_to_hidden1')),\n",
    "    # Create another hidden layer with 128 neurons\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu, name='hidden1_to_hidden2')),\n",
    "    # Create an \"output layer\" with 10 neurons\n",
    "    model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax, name='hidden_to_logits')),\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001),  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['SparseCategoricalCrossentropy', 'accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=50, batch_size=32, verbose=0,  validation_split = 0.3, callbacks=[tqdm_callback ])\n",
    "\n",
    "# hist = pd.DataFrame(history.history)\n",
    "# hist['epoch'] = history.epoch\n",
    "# hist.tail()\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMhW79hT9fa5"
   },
   "source": [
    "**Adjusting our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHi3qIH28Hjt"
   },
   "outputs": [],
   "source": [
    "# we need to limit our epochs to 15 so that the model would not overfit!!!\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=32, verbose=0,  validation_split = 0.3, callbacks=[tqdm_callback ])\n",
    "\n",
    "# hist = pd.DataFrame(history.history)\n",
    "# hist['epoch'] = history.epoch\n",
    "# hist.tail()\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_nYV2ao-XHg"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms_cFmIK-U_m"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(test_images.shape)\n",
    "_,test_loss, test_accuracy= model.evaluate(test_images,test_labels)\n",
    "print('Completed testing on', test_images.shape[0], 'examples...')\n",
    "print('Loss: {:.3f}, Accuracy: {:.3%}'.format(test_loss, test_accuracy))\n",
    "test_predictions = tf.argmax(model.predict(test_images), axis=1, output_type=tf.int32)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = np.random.randint(0, 10000)\n",
    "    plt.imshow(test_images[img_index], cmap=\"gray_r\")\n",
    "\n",
    "    actual_label = test_labels[img_index]\n",
    "    predicted_label = int(test_predictions[img_index])\n",
    "\n",
    "    plt.xlabel(\"Actual: {} ({})\\n Predicted: {} ({})\".format( actual_label, text_labels[actual_label], predicted_label, text_labels[predicted_label]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "k180211.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
